%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% background_relatedwork.tex
%% NOVA thesis document file
%%
%% Chapter with Background and Related Work
%% REVIEWED by Diogo Gato 2022/03/29
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% REVER SE H√Å OLD ARTIGOS QUE SEJAM IMPORTANTES MENCIONAR AQUI E USAR

\typeout{NT FILE literature.tex}

\chapter{State of the Art}
\label{cha:introduction}


\section{Introduction} 
\label{sub:if_you_use_this_template} 

In this chapter it will be presented a review on the articles that we consider as basis to support the methodology chosen. We will divide this section into several parts, according to the steps that a machine learning model is developed.
We will first introduce the references that were used to auxiliary the decision of our dataset and the pre-processing mechanisms that were decided. Then we will advance to the articles that support our feature engineering process. We advance to the machine learning algorithms that were chosen and conclude with the references that support the evaluation mechanisms that were decided to be used in order to rank our training models.

\section{Literature Analysis}


\subsection{Features}

In several articles like [N1 (WIND)] [N5 (WIND)] [...], and as stated by CGI experts, vibration signals are one of the most important features that can be used in failure prediction in mechanical systems. In our particular case, we don't have access to vibration signals.
In [19 (WIND)], for the prediction of "high temperature fault", it is presented in Table 1 several signals that it was used in this article, in which we can see that several temperature signals from several wind turbine components were chosen as well as signals that provide general information about the state of a wind turbine (like rotor speed, blade pitch angle and power).
In [N2 (WIND)] and [39 (WIND)], several sensors with generic information about a wind turbine were used for the fault detection. These sensors provide information like generated electrical power, rotor speed, generator speed and pitch angle. It is stated also that "the main challenges of the wind turbine fault detection lie in their non-linearity, unknown disturbances, and significant measurement noise at each sensor", providing an important feedback concerning the difficulty to detect standards between failures and specific component states.
[N3 (WIND)] studies two failures, one related to blade angle and another concerning generator. For each failure, the features selected were some general ones (like rotor speed, generator speed, wind speed), some of temperature concerning the second failure and some specific of the component of the failure, like specific component sensors or sensors of components that are related (for the blade angle failure, blade and nacelle sensors; for the generator failure, generator, bearing and nacelle sensors).
In [41 (WIND)], it is considered as a parameter related to the fault the generator speed, power output and wind speed.
Finally, in [N8 (WIND)], for the prediction of generator and gearbox failures, mostly all the sensors concerning temperatures and oil temperatures were used as well as general ambient and wind turbine information like average wind speed, total production, ambient temperature.


\subsection{Dataset and Feature Engineering}
%Colocar nota sobre "Inequality Method of Checbyshev - Remove data outliers
%TODO: Procurar o site onde vi que o ChiSquared era melhor para classificacao

Classification problems, more particularly fault prediction problems, have several dataset pre-processing work that is make in order to improve the performance of the fault detection. In almost all of our literature that present some dataset information, we see some "standards" in the approaches that are used.

In [19 (WIND)], it is referred the removal of data outliers, by using Chebyshev inequality method, the scaling of the features, using normalization and after selecting a subset of signals that are related to the failure, it is performed a feature selection using a feature ranking using correlation coefficient. It also uses a sliding time window on the features.

In [N2 (WIND)], it is pre-processed the features by "group scaling and feature transformation", reducing the dimensionality of the feature space using "multiway principal component analysis". The next step is to "10-fold cross-validation" using SVM based classification, in order to assess if results will generalize to an independent dataset. This article also refer that "methods that focus on a specific part of the WT (Wind Turbine), require the choice of the most appropriate sensors, their advisable position in the sub-assembly".

[39 (WIND)] only refers the importance of reducing a large number of features and the use of feature extraction and feature selection techniques. It uses also the 10-fold cross-validation for data split.

[N3 (WIND)] and [41 (WIND)] states that in a study like ours, of failure prediction, there exists an unbalanced dataset. The number of faults events is much lower that the number of normal state events. That is something that should be treated on the training dataset to balance the dataset that is used to train the model.
[N4 (WIND)] uses, for feature selection, Pearson correlation to eliminate the more correlated features and then use "PCA to identify the variables that exceeds the threshold".

[TDC (1)] and [MED(1)] highlight one of the most important dataset transformations that can be used in machine learning problems such as the one of our master thesis. It refers for the dataset sliding window technique that allow the machine learning model to learn the behaviour of the wind turbine a set of time before the failure actually occur.

Other important technique referred by [MED (1)], and more briefly by [N7 (GENERAL)] is the running summaries. This consists in building new features from an statistical aggregation (for example, average) of the last set of values of each feature. This allow us to compare the current value of the feature with the average behaviour of the feature, discovering values that are not considered as normal and thus indicating a possible failure.

In addition, [MED (1)] describes also some data pre-processing techniques that were also referred in the previous articles, like the removal of the null and duplicated rows, ways to deal with the split of data for the train/validation/test datasets.
[Machine Learning Mystery - Train-Test-Split] explains the importance of the Train-Test Split for evaluating machine learning algorithms in comparison with other techniques. Since we have a dataset with a sufficient high size, this article recommends the use of this technique to evaluate the algorithms.


\subsection{Machine Learning Algorithm's}

The machine learning algorithms that are mentioned by our literature to address our master thesis theme, are the same across all the literature.

We have to mention that not all of the articles use exactly the same techniques to predict failures as we use, and this have influence on the algorithms that are chosen. Another important factor is that we are restricted to use the algorithms that are available in Azure Machine Learning Studio, factor that influence the use of the algorithms and their parameters, thus affecting their performance.

Neural Network are mentioned in [15 (WIND)], [20 (WIND)], [41 (WIND)] and [N8 (WIND)]. Decision Tree algorithms are analyzed in [41 (WIND)], [39 (WIND)], [N3 (WIND)] and [N7 (WIND)]. Logistic Regression ( [N5 (WIND)], [N7 (WIND)], [N8 (WIND)] and K-Nearest Neighbors ( [39 (WIND)], [N3 (WIND)], [N4 (WIND)] are more simple algorithms that are also used but the one that is more mentioned in our literature is the Support Vector Machine algorithm, which is mentioned in [41 (WIND)], [19 (WIND)], [39 (WIND)], [N1 (WIND)], [N2 (WIND)], [N3 (WIND)] and [N8 (WIND)].

Articles like [ML Alg Analysis 2] and [ML Alg Analysis] also mention in detail some of these algorithms as the ones to be used for classification problems. Some of the algorithms mentioned by these two articles are K-Nearest Neighbor, Logistic Regression, Decision Tree Classifier, Random Forest Classifier, Support Vector Machine and Naive Bayes Classifier.

One algorithm that is also mentioned and that is said to be one of the most adequate for our problem in particular, is the Long Short Term Memory (LSTM) [TDC (1)]. Although0p0 this analysis from our literature, we will not use it because it is not available in Azure Machine Learning Studio.


\subsection{Evaluation Metrics}

For binary classification problems, general literature about the machine learning area and our algorithms have the same opinion of the metrics that should be used. Of course this metrics that are evaluated are in general terms, without any specific consideration in terms of business needs. In our particular case, we have a particular interest in reducing the number of false positives, even if that lead to a higher number of false negatives and a lower number of true positives. This idea is justified in more detail in the next chapters.

[31 (WIND)] mention the use of the accuracy, false positive rate, true positive rate, ROC curve and confusion matrix. [18 (WIND)] uses ROC curve and AUC curve. 
[N3 (WIND)] uses ACC (classification accuracy), SEN (sensitivity) and SPE (specifity), which are metrics that are derived from the confusion metrics and that provide us percentual values.

[41 (WIND)], [ML Alg Analysis 2], uses the most common metrics for evaluating binary classification problems: true positives, false negatives, true negatives and false positives, and percentage values that are calculated from the previous mention values, accuracy, recall, precision and F1 score.

In [MED(1)] and backed up by our CGI expert, we can't rely only on these metrics to evaluate a running algorithm when we are using the sliding time window technique. For this case, we want to build our own evaluation technique that should rely on "additional logic and business heuristics" [MED(1)] to understand if in a time window, the fault was predicted or not, and not only if the algorithm predicts all the timestamps that were flagged as belonging to the sliding time window of a failure. More on these heuristic will be explained in the next chapters.


\section{Conclusion} 
\label{sub:if_you_use_this_template} 

Considering the analysis that was made on the articles previously mentioned and the advices from CGI experts, we built our methodology to address the problem of our master thesis with the technology that we have available.

%% TODO: CONFIRMAR PELO EXCEL SE NAO SE COLOCOU PARA TODAS AS FALHAS OS SINAIS DA TEMPERATURA DO OLEO. OU SE FOI APENAS ESPECIFICO DE ALGUMAS FALHAS

In terms of the features that were selected for each fault, it was decided to use general data signals, like wind speed, temperature, active power of the wind turbine for all the faults. Then, for specific signals of each fault, we selected the signals of the fault component or components that were neighbors of the fault components.

In terms of the dataset and feature engineering step, considering the limitations of the technology that we have to use, the 10 fold cross-validation was used in order to understand the quality of our model in terms of predicting new data. For the feature selection, we used the Filter Based Feature Selection module, with the Chi squared metric. The Principal Component Analysis was deprecated due to limitations of the Azure Machine Learning Studio that doesn't have a built-in module to use the PCA. In the built of the dataset, we remove invalid rows (rows which the features are null) and features (when the feature have to much null values), select only data that are considered to have quality (meaning that have no noise) and build the running summaries of the features. Further one, we use the train-test split to train, validate and test our model.

In terms of the machine learning algorithms, and considering all the machine learning alghoritms available in the Azure Machine Learning Studio, we selected the Two-Class Logistic Regression, Two-Class Support Vector Machine, Two-Class Neural Network, Two-Class Decision Tree and Two-Class Boosted Decision Tree.

Finally, to evaluate our models, we use as metrics Precision, Recall and F1 Score. Also considering the [MED(1)], we built a script to understand the real capacity of the model in predicting faults and run it into a dataset that was not used in the build of the model.